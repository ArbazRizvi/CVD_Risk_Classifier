# In[1]:
# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Import Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

# Import Metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Set plot style
sns.set(style="whitegrid")


# In[2]:
## 1. Data Loading and Initial Exploration
print("Loading data...")
try:
    data = pd.read_csv('data/framingham.csv')
except FileNotFoundError:
    print("Error: 'data/framingham.csv' not found.")
    print("Please download the dataset and place it in the 'data/' directory.")
    # Exit or raise error if in a script
    # For a notebook, we'll stop execution here.
    assert False, "Dataset not found."

print(data.head())
print("\nData Info:")
data.info()
print("\nMissing values:")
print(data.isnull().sum())


# In[3]:
## 2. Data Preprocessing
print("\nStarting preprocessing...")

# Define categorical and numerical features
# 'male' and 'currentSmoker' are already 0/1, but we'll list them to be safe.
# 'education' is categorical.
categorical_features = ['education']

# All other features are numerical, except the target
numerical_features = [col for col in data.columns if col not in ['education', 'TenYearCHD']]

# Drop 'education' if it's not present in your specific CSV version, or handle it.
# For this example, we assume 'education' is a categorical feature to be one-hot encoded.
# If 'education' isn't useful, you could just drop it:
# data = data.drop('education', axis=1)
# numerical_features.remove('education') # if it was in the list

# Separate features (X) and target (y)
X = data.drop('TenYearCHD', axis=1)
y = data['TenYearCHD']

# --- Handle Missing Values ---
# We will use median imputation for all features.
# This is a simple and robust strategy.
print("Handling missing values using median imputation...")
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# Convert back to DataFrame to keep column names
X = pd.DataFrame(X_imputed, columns=X.columns)

# --- Define Preprocessing Pipelines ---

# Pipeline for numerical features:
# 1. StandardScaler
numeric_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

# Pipeline for categorical features:
# 1. OneHotEncoder
categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# --- Create the ColumnTransformer ---
# This applies the correct transformer to the correct columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep any columns not listed
)


# In[4]:
## 3. Split Data
print("Splitting data into training and testing sets...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
print(f"Train set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")


# In[5]:
## 4. Model Training and Evaluation
# We will create a dictionary of models to train

models = {
    "Logistic Regression": LogisticRegression(solver='liblinear', random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Support Vector Machine": SVC(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42)
}

# Dictionary to store results
results = {}

print("\nTraining and evaluating models...")

for name, model in models.items():
    # Create the full pipeline: Preprocessor -> Model
    clf_pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', model)
    ])
    
    # Train the model
    clf_pipeline.fit(X_train, y_train)
    
    # Make predictions
    y_pred = clf_pipeline.predict(X_test)
    
    # Evaluate
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    
    # Store results
    results[name] = [accuracy, precision, recall, f1]
    
    print(f"\n--- {name} ---")
    print(classification_report(y_test, y_pred))


# In[6]:
## 5. Compare Models
print("\nModel Comparison:")
results_df = pd.DataFrame(results, index=["Accuracy", "Precision", "Recall", "F1-Score"]).T
results_df = results_df.sort_values(by="F1-Score", ascending=False)
print(results_df)

# Plotting the results
results_df.plot(kind='bar', figsize=(12, 7))
plt.title('Model_Comparison_Metrics')
plt.ylabel('Score')
plt.xticks(rotation=0)
plt.legend(loc='lower right')
plt.show()

print("\nRandom Forest shows the best balance, especially on F1-Score. We will tune this model.")


# In[7]:
## 6. Hyperparameter Tuning (Random Forest)
print("\nStarting Hyperparameter Tuning for Random Forest...")

# Create the pipeline
rf_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Define the parameter grid
# Note: 'classifier__' prefix is needed to pass params to the 'classifier' step
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2]
}

# Use F1-Score as the scoring metric
grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train, y_train)

print(f"\nBest parameters found: {grid_search.best_params_}")
print(f"Best F1-score from tuning: {grid_search.best_score_:.4f}")


# In[8]:
## 7. Evaluate the Optimized Model
print("\nEvaluating the optimized Random Forest model on the test set:")

# Get the best estimator
best_rf = grid_search.best_estimator_

# Make predictions on the test set
y_pred_best = best_rf.predict(X_test)

# Calculate final metrics
accuracy_best = accuracy_score(y_test, y_pred_best)
precision_best = precision_score(y_test, y_pred_best)
recall_best = recall_score(y_test, y_pred_best)
f1_best = f1_score(y_test, y_pred_best)

print(f"Optimized Model Accuracy: {accuracy_best:.4f}")
print(f"Optimized Model Precision: {precision_best:.4f}")
print(f"Optimized Model Recall: {recall_best:.4f}")
print(f"Optimized Model F1-Score: {f1_best:.4f}")

print("\nFinal Classification Report:")
print(classification_report(y_test, y_pred_best))

# Note: Achieving an 89% F1-score as stated in the prompt is very high for this dataset.
# The results from this standard pipeline will be more realistic (often much lower).
# To match the 89% F1, you may need more advanced feature engineering, 
# different imputation (like MICE), or synthetic data (SMOTE) to handle imbalance.
# This code strictly follows the described methodology.

print("\n--- Project Complete ---")
